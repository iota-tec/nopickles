{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from transformers import BertTokenizer, TFBertForTokenClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# os.chdir('/content/drive/Othercomputers/AKATSUKI-PC/PycharmProjects/chatopotamus')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T19:25:02.706539700Z",
     "start_time": "2023-11-20T19:24:51.306075300Z"
    }
   },
   "id": "94a0413a7a2ffe33"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "\n",
    "# Load files\n",
    "with open('resources/bert/data/ner_dataset.json', 'r') as file:\n",
    "\tner_dataset_json = json.load(file)\n",
    "\n",
    "with open('resources/bert/data/ner_lookup.json', 'r') as file:\n",
    "\tner_lookup = json.load(file)\n",
    "\n",
    "num_labels = len(ner_lookup['entities'])*2 + 1\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = TFBertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T19:26:29.867160600Z",
     "start_time": "2023-11-20T19:26:23.036503Z"
    }
   },
   "id": "638a160f14d658b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Convert dataset into IOB tagged data**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e459d9ac37544e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_IOB(dataset, lookup):\n",
    "    iob_data = []\n",
    "\n",
    "    # Split entities into one-word and two-word groups\n",
    "    one_word_entities = {}\n",
    "    two_word_entities = {}\n",
    "    for type, values in lookup['entities'].items():\n",
    "        one_word_entities[type] = []\n",
    "        two_word_entities[type] = []\n",
    "        for value in values:\n",
    "            entity_split = value.split(' ')\n",
    "            if len(entity_split) == 1:\n",
    "                one_word_entities[type].append(entity_split[0].lower())\n",
    "            else:\n",
    "                two_word_entities[type].append(' '.join(entity_split).lower())\n",
    "\n",
    "    for instance in dataset['instances']:\n",
    "        original_sentence = instance['sentence']\n",
    "        # Remove punctuation for matching, but keep the original sentence for output\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', original_sentence)\n",
    "        words = sentence.split(' ')\n",
    "        words_lower = [word.lower() for word in words]\n",
    "        this_sentence_tags = ['O'] * len(words)\n",
    "\n",
    "        # Function to update tags for a given entity type\n",
    "        def update_tags(entity_values, tag_prefix):\n",
    "            for entity in entity_values:\n",
    "                start_index = 0\n",
    "                while start_index < len(words_lower):\n",
    "                    try:\n",
    "                        start_index = words_lower.index(entity.split(' ')[0], start_index)\n",
    "                        end_index = start_index + len(entity.split(' '))\n",
    "                        if ' '.join(words_lower[start_index:end_index]) == entity:\n",
    "                            this_sentence_tags[start_index] = f'B-{tag_prefix}'\n",
    "                            for i in range(start_index + 1, end_index):\n",
    "                                this_sentence_tags[i] = f'I-{tag_prefix}'\n",
    "                        start_index = end_index\n",
    "                    except ValueError:\n",
    "                        break\n",
    "\n",
    "        # Check for two-word entities\n",
    "        for entity_type, values in two_word_entities.items():\n",
    "            update_tags(values, entity_type)\n",
    "\n",
    "        # Check for one-word entities\n",
    "        for entity_type, values in one_word_entities.items():\n",
    "            for i, word in enumerate(words_lower):\n",
    "                if word in values and this_sentence_tags[i] == 'O':\n",
    "                    this_sentence_tags[i] = f'B-{entity_type}'\n",
    "\n",
    "        # Pair the original words (with punctuation) with their IOB tags\n",
    "        original_words = original_sentence.split(' ')\n",
    "        iob_data.append((original_sentence, this_sentence_tags))\n",
    "\n",
    "    return iob_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T19:26:29.885166700Z",
     "start_time": "2023-11-20T19:26:29.865158600Z"
    }
   },
   "id": "24149f37a81d0bc5"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "iob_data = convert_to_IOB(ner_dataset_json, ner_lookup)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T19:26:29.965273100Z",
     "start_time": "2023-11-20T19:26:29.871166700Z"
    }
   },
   "id": "650af7ed0f1767d1"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "('Give me a hot chocolate, no whipped topping, and a grilled cheese sandwich.',\n ['O',\n  'O',\n  'O',\n  'B-beverage',\n  'I-beverage',\n  'O',\n  'B-beverage_modifier',\n  'I-beverage_modifier',\n  'O',\n  'O',\n  'B-food',\n  'I-food',\n  'O'])"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iob_data[2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T19:26:45.799184400Z",
     "start_time": "2023-11-20T19:26:45.766177200Z"
    }
   },
   "id": "d15fab5b77944c40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tokenization and Handling Subword Tokens**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9f7ccab7b593a9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**For Reloading/Saving**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5c00294ec543ee"
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "# Save important variables\n",
    "joblib.dump(iob_data, 'resources/bert/data/iob_data.pkl')\n",
    "joblib.dump(tokenizer, 'resources/bert/pretrained/tokenizer.pkl')\n",
    "model.save_weights('resources/bert/pretrained/bert-base-uncased.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T07:09:02.429795700Z",
     "start_time": "2023-11-20T07:08:59.488943400Z"
    }
   },
   "id": "4f40a4d09d32544f"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x000002D6C13D5D60>.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Load variables\u001B[39;00m\n\u001B[0;32m      2\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m joblib\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresources/bert/pretrained/tokenizer.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mresources/bert/pretrained/bert-base-uncased.h5\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m iob_dataset \u001B[38;5;241m=\u001B[39m joblib\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresources/bert/data/iob_dataset.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Iota\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\Iota\\lib\\site-packages\\keras\\saving\\hdf5_format.py:194\u001B[0m, in \u001B[0;36mload_model_from_hdf5\u001B[1;34m(filepath, custom_objects, compile)\u001B[0m\n\u001B[0;32m    192\u001B[0m model_config \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mattrs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel_config\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    193\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m model_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 194\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    195\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo model config found in the file at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfilepath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    196\u001B[0m     )\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model_config, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdecode\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m    198\u001B[0m     model_config \u001B[38;5;241m=\u001B[39m model_config\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x000002D6C13D5D60>."
     ]
    }
   ],
   "source": [
    "# Load variables\n",
    "tokenizer = joblib.load('resources/bert/pretrained/tokenizer.pkl')\n",
    "model = keras.models.load_weights('resources/bert/pretrained/bert-base-uncased.h5')\n",
    "iob_dataset = joblib.load('resources/bert/data/iob_dataset.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T19:25:35.880333900Z",
     "start_time": "2023-11-20T19:25:33.014100600Z"
    }
   },
   "id": "83107583914355c9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
