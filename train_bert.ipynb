{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import keras.models\n",
    "\n",
    "os.chdir('/content/drive/Othercomputers/AKATSUKI-PC/PycharmProjects/chatopotamus')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T06:42:49.345512600Z",
     "start_time": "2023-11-18T06:42:49.343513300Z"
    }
   },
   "id": "94a0413a7a2ffe33"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def convert_to_IOB(data, lookup):\n",
    "\tconverted_data = []\n",
    "\n",
    "\tfor instance in data['instances']:\n",
    "\t\tsentence = instance['sentence']\n",
    "\t\twords = sentence.split()\n",
    "\n",
    "\t\t# Initialize tags as 'O' for Outside\n",
    "\t\ttags = ['O'] * len(words)\n",
    "\n",
    "\t\t# Function to tag words based on entities\n",
    "\t\tdef tag_words(entity_type, entity_values):\n",
    "\t\t\tfor entity_value in entity_values:\n",
    "\t\t\t\tstart = sentence.find(entity_value)\n",
    "\t\t\t\tif start != -1:\n",
    "\t\t\t\t\t# Calculate end position of the entity\n",
    "\t\t\t\t\tend = start + len(entity_value)\n",
    "\t\t\t\t\tstart_word_count = len(sentence[:start].split())\n",
    "\t\t\t\t\tend_word_count = len(sentence[:end].split())\n",
    "\t\t\t\t\tfor i in range(start_word_count, end_word_count):\n",
    "\t\t\t\t\t\tif i == start_word_count:\n",
    "\t\t\t\t\t\t\ttags[i] = 'B-' + entity_type\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\ttags[i] = 'I-' + entity_type\n",
    "\n",
    "\t\t# Tagging each entity type\n",
    "\t\tfor entity_type, entity_values in lookup['entities'].items():\n",
    "\t\t\ttag_words(entity_type, entity_values)\n",
    "\n",
    "\t\tconverted_data.append({\"sentence\": sentence, \"tags\": tags})\n",
    "\n",
    "\treturn converted_data\n",
    "\n",
    "# Example usage\n",
    "with open('resources/bert/data/ner_dataset.json', 'r') as file:\n",
    "\tner_dataset_json = json.load(file)\n",
    "\n",
    "with open('resources/bert/data/ner_lookup.json', 'r') as file:\n",
    "\tner_lookup = json.load(file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T06:58:45.473059400Z",
     "start_time": "2023-11-18T06:58:44.653815500Z"
    }
   },
   "id": "d43f373f24c7df65"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "iob_dataset = convert_to_IOB(ner_dataset_json, ner_lookup)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T07:22:24.218336Z",
     "start_time": "2023-11-18T07:22:24.213334100Z"
    }
   },
   "id": "b6df259892833e88"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForTokenClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "num_labels = len(ner_lookup['entities'])*2 + 1\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = TFBertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bced9c2896d45ae4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**For Reloading/Saving**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5c00294ec543ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save important variables\n",
    "joblib.dump(iob_dataset, 'resources/bert/data/iob_dataset.pkl')\n",
    "joblib.dump(tokenizer, 'resources/bert/pretrained/tokenizer.pkl')\n",
    "model.save('resources/bert/pretrained/bert-base-uncased.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f40a4d09d32544f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load variables\n",
    "tokenizer = joblib.load('resources/bert/pretrained/tokenizer.pkl')\n",
    "model = keras.models.load_model('resources/bert/pretrained/bert-base-uncased.keras')\n",
    "iob_dataset = joblib.load('resources/bert/data/iob_dataset.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83107583914355c9"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "data": {
      "text/plain": "{'sentence': 'i ordered a medium double-double and a boston cream donut from tim hortons',\n 'tags': ['O',\n  'O',\n  'O',\n  'B-beverage_size',\n  'O',\n  'O',\n  'O',\n  'O',\n  'O',\n  'B-food',\n  'O',\n  'O',\n  'O']}"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iob_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-18T07:57:14.807631300Z",
     "start_time": "2023-11-18T07:57:14.592077400Z"
    }
   },
   "id": "9dbc32590a60b1e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "92ec7195fe498011"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
