{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from transformers import BertTokenizer, TFBertForTokenClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# os.chdir('/content/drive/Othercomputers/AKATSUKI-PC/PycharmProjects/chatopotamus')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T06:47:31.898842200Z",
     "start_time": "2023-11-20T06:47:31.873040600Z"
    }
   },
   "id": "94a0413a7a2ffe33"
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [],
   "source": [
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', use_fast=True)\n",
    "\n",
    "# Load files\n",
    "with open('resources/bert/data/ner_dataset.json', 'r') as file:\n",
    "\tner_dataset_json = json.load(file)\n",
    "\n",
    "with open('resources/bert/data/ner_lookup.json', 'r') as file:\n",
    "\tner_lookup = json.load(file)\n",
    "\n",
    "num_labels = len(ner_lookup['entities'])*2 + 1\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = TFBertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T06:49:30.001728600Z",
     "start_time": "2023-11-20T06:49:28.840975400Z"
    }
   },
   "id": "638a160f14d658b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Convert dataset into IOB tagged data**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e459d9ac37544e"
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_to_IOB(dataset, lookup):\n",
    "    iob_data = []\n",
    "\n",
    "    # Split entities into one-word and two-word groups\n",
    "    one_word_entities = {}\n",
    "    two_word_entities = {}\n",
    "    for type, values in lookup['entities'].items():\n",
    "        one_word_entities[type] = []\n",
    "        two_word_entities[type] = []\n",
    "        for value in values:\n",
    "            entity_split = value.split(' ')\n",
    "            if len(entity_split) == 1:\n",
    "                one_word_entities[type].append(entity_split[0].lower())\n",
    "            else:\n",
    "                two_word_entities[type].append(' '.join(entity_split).lower())\n",
    "\n",
    "    for instance in dataset['instances']:\n",
    "        original_sentence = instance['sentence']\n",
    "        # Remove punctuation for matching, but keep the original sentence for output\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', original_sentence)\n",
    "        words = sentence.split(' ')\n",
    "        words_lower = [word.lower() for word in words]\n",
    "        this_sentence_tags = ['O'] * len(words)\n",
    "\n",
    "        # Function to update tags for a given entity type\n",
    "        def update_tags(entity_values, tag_prefix):\n",
    "            for entity in entity_values:\n",
    "                start_index = 0\n",
    "                while start_index < len(words_lower):\n",
    "                    try:\n",
    "                        start_index = words_lower.index(entity.split(' ')[0], start_index)\n",
    "                        end_index = start_index + len(entity.split(' '))\n",
    "                        if ' '.join(words_lower[start_index:end_index]) == entity:\n",
    "                            this_sentence_tags[start_index] = f'B-{tag_prefix}'\n",
    "                            for i in range(start_index + 1, end_index):\n",
    "                                this_sentence_tags[i] = f'I-{tag_prefix}'\n",
    "                        start_index = end_index\n",
    "                    except ValueError:\n",
    "                        break\n",
    "\n",
    "        # Check for two-word entities\n",
    "        for entity_type, values in two_word_entities.items():\n",
    "            update_tags(values, entity_type)\n",
    "\n",
    "        # Check for one-word entities\n",
    "        for entity_type, values in one_word_entities.items():\n",
    "            for i, word in enumerate(words_lower):\n",
    "                if word in values and this_sentence_tags[i] == 'O':\n",
    "                    this_sentence_tags[i] = f'B-{entity_type}'\n",
    "\n",
    "        # Pair the original words (with punctuation) with their IOB tags\n",
    "        original_words = original_sentence.split(' ')\n",
    "        iob_data.append((original_sentence, this_sentence_tags))\n",
    "\n",
    "    return iob_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T06:36:59.772217600Z",
     "start_time": "2023-11-20T06:36:59.749521600Z"
    }
   },
   "id": "24149f37a81d0bc5"
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "iob_data = convert_to_IOB(ner_dataset_json, ner_lookup)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T06:37:01.563020100Z",
     "start_time": "2023-11-20T06:37:01.440978800Z"
    }
   },
   "id": "650af7ed0f1767d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "iob_data[0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d15fab5b77944c40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tokenization and Handling Subword Tokens**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9f7ccab7b593a9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**For Reloading/Saving**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5c00294ec543ee"
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "outputs": [],
   "source": [
    "# Save important variables\n",
    "joblib.dump(iob_data, 'resources/bert/data/iob_data.pkl')\n",
    "joblib.dump(tokenizer, 'resources/bert/pretrained/tokenizer.pkl')\n",
    "model.save_weights('resources/bert/pretrained/bert-base-uncased.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T07:09:02.429795700Z",
     "start_time": "2023-11-20T07:08:59.488943400Z"
    }
   },
   "id": "4f40a4d09d32544f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load variables\n",
    "tokenizer = joblib.load('resources/bert/pretrained/tokenizer.pkl')\n",
    "model = keras.models.load_model('resources/bert/pretrained/bert-base-uncased.h5')\n",
    "iob_dataset = joblib.load('resources/bert/data/iob_dataset.pkl')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83107583914355c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a6893e8cadd8c86d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
