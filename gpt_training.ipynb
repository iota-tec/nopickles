{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-20T19:48:54.685860600Z",
     "start_time": "2023-11-20T19:48:47.889596Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c19477c69ff5c57b"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "text_path = 'resources/gpt/data/conversational_data.txt'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T21:04:36.455765500Z",
     "start_time": "2023-11-20T21:04:36.440676900Z"
    }
   },
   "id": "5c997b8629569d33"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def process_conversational_data(file_path):\n",
    "    sets_of_eight = []\n",
    "    current_set = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        all_lines = f.readlines()\n",
    "\n",
    "    for i, line in enumerate(all_lines):\n",
    "        # Trim the newline character from the end of the line and convert to lower case\n",
    "        line = line.rstrip('\\n').lower()\n",
    "\n",
    "        if i > 0:\n",
    "            # Extract the first word (speaker) of the current and previous lines\n",
    "            speaker_current = line.split(':')[0]\n",
    "            speaker_previous = all_lines[i - 1].rstrip('\\n').lower().split(':')[0]\n",
    "\n",
    "            # Check if the current line has a different speaker than the previous line\n",
    "            if speaker_current != speaker_previous:\n",
    "                current_set.append(line)\n",
    "\n",
    "                # Add the set to sets_of_eight every 8 lines\n",
    "                if len(current_set) == 8:\n",
    "                    sets_of_eight.append(current_set)\n",
    "                    current_set = []\n",
    "            else:\n",
    "                # If the same speaker is found consecutively, reset the current set\n",
    "                current_set = [line]  # Start a new set with the current line\n",
    "        else:\n",
    "            # For the very first line, just add it to the current set\n",
    "            current_set.append(line)\n",
    "\n",
    "    # Check if there's a leftover set with less than 8 lines\n",
    "    if current_set:\n",
    "        sets_of_eight.append(current_set)\n",
    "\n",
    "    return sets_of_eight"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T21:04:37.106759800Z",
     "start_time": "2023-11-20T21:04:37.082219400Z"
    }
   },
   "id": "e8625e4dd233e55a"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "sets_of_eight = process_conversational_data(text_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T21:06:40.465971900Z",
     "start_time": "2023-11-20T21:06:40.417894300Z"
    }
   },
   "id": "d6cd0dbbe31b3baa"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "def prepare_for_gpt2(sets_of_eight):\n",
    "    processed_sequences = []\n",
    "    for set in sets_of_eight:\n",
    "        # Concatenate the lines in the set into a single string\n",
    "        concatenated_sequence = \" \".join(set)\n",
    "        # Add the GPT-2 end-of-text token\n",
    "        sequence_with_token = f\"{concatenated_sequence} <|endoftext|>\"\n",
    "        processed_sequences.append(sequence_with_token)\n",
    "    return processed_sequences\n",
    "\n",
    "# Example usage\n",
    "gpt2_ready_sequences = prepare_for_gpt2(sets_of_eight)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T21:13:12.454747700Z",
     "start_time": "2023-11-20T21:13:12.382597Z"
    }
   },
   "id": "c0d4ef5feb430422"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gpt2_ready_sequences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "287e3cd041d18032"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_inputs = tokenizer(gpt2_ready_sequences, truncation=True, padding=True, return_tensors=\"tf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T21:27:17.178587600Z",
     "start_time": "2023-11-20T21:27:15.268348600Z"
    }
   },
   "id": "b288b2c3602e1cf"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "['tokenized_inputs.pkl']"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(tokenized_inputs, 'tokenized_inputs.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-20T21:38:53.535757400Z",
     "start_time": "2023-11-20T21:38:53.485338100Z"
    }
   },
   "id": "df7e72959ed311f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
